# RL4AMOD with Penalty-Based Soft Constraint

> **Fork of [StanfordASL/RL4AMOD](https://github.com/StanfordASL/RL4AMOD)** with penalty-based soft constraint for rebalancing optimization.

## ğŸ¯ Key Modification

### Problem
Original RL4AMOD forces LP to strictly achieve RL's target vehicle distribution, causing **excessive rebalancing** without considering time/opportunity costs.

### Solution
Introduced **Soft Constraint with Penalty** that allows LP to perform cost-benefit analysis:

```
Objective: minimize( rebalancing_cost + shortage_penalty Ã— unmet_target )
```

- LP can skip expensive rebalancing if penalty cost is lower
- `shortage_penalty` parameter controls the trade-off

---

## ğŸ“ Project Structure

```
RL4AMOD_penalty/
â”œâ”€â”€ train.py                    # Training script
â”œâ”€â”€ testing.py                  # Testing script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ algos/
â”‚   â”‚   â”œâ”€â”€ sac.py              # SAC algorithm (main RL agent)
â”‚   â”‚   â”œâ”€â”€ reb_flow_solver.py  # LP solver interface (passes penalty)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ cplex_mod/
â”‚   â”‚   â”œâ”€â”€ minRebDistRebOnly.mod  # â­ Modified LP with soft constraint
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”‚   â””â”€â”€ sac.yaml        # SAC hyperparameters
â”‚   â”‚   â””â”€â”€ simulator/
â”‚   â”‚       â”œâ”€â”€ macro.yaml      # Macro simulator config
â”‚   â”‚       â””â”€â”€ sumo.yaml       # SUMO simulator config (+ shortage_penalty)
â”‚   â””â”€â”€ envs/
â”‚       â””â”€â”€ sim/
â”‚           â”œâ”€â”€ macro_env.py    # Macroscopic simulator
â”‚           â””â”€â”€ sumo_env.py     # SUMO-based mesoscopic simulator
â””â”€â”€ saved_files/                # Logs, checkpoints, results
```

---

## ğŸš€ Quick Start (Macro Environment)

### Prerequisites

```bash
pip install -r requirements.txt
```

> **Note**: CPLEX is recommended but optional. Without CPLEX, PuLP solver is used automatically.

### Training

```bash
# Basic training with SAC
python train.py simulator=macro model=sac

# Specify city and checkpoint path
python train.py simulator=macro model=sac simulator.city=nyc_brooklyn model.checkpoint_path=SAC_penalty

# Adjust training episodes
python train.py simulator=macro model=sac model.max_episodes=5000
```

### Testing

```bash
# Test trained model
python testing.py simulator=macro model=sac model.checkpoint_path=SAC_penalty

# Test with specific episodes
python testing.py simulator=macro model=sac model.test_episodes=20
```

### Key Parameters (Macro)

| Parameter | Default | Description |
|-----------|---------|-------------|
| `simulator.city` | `nyc_brooklyn` | City dataset |
| `simulator.demand_ratio` | `0.5` | Demand scaling factor |
| `simulator.beta` | `0.5` | Rebalancing cost coefficient |
| `simulator.max_steps` | `20` | Steps per episode |
| `model.max_episodes` | `10000` | Training episodes |
| `model.batch_size` | `100` | Batch size |

---

## âš™ï¸ Penalty Parameter Tuning

Configure in `src/config/simulator/sumo.yaml`:

```yaml
shortage_penalty: 3.0  # Default value
```

Or override via command line:
```bash
python train.py simulator=sumo simulator.shortage_penalty=5.0
```

| Value | Behavior |
|-------|----------|
| 0.5 ~ 2.0 | Conservative rebalancing (cost priority) |
| 3.0 ~ 5.0 | Balanced trade-off |
| 10.0+ | Aggressive rebalancing (target priority) |

---

## ğŸ“Š LP Model Details

### Modified Objective Function
```
minimize(
  Î£ rebFlow[e] Ã— time[e]              // Rebalancing cost
  + shortage_penalty Ã— Î£ shortage[i]  // Penalty for unmet targets
)
```

### Soft Constraint
```
net_inflow[i] + shortage[i] >= desiredVehicles[i] - currentVehicles[i]
```

The `shortage[i]` slack variable allows regions to fall short of their target when rebalancing is too expensive.

---

## ğŸ“ Citation

If you use this code, please cite the original work:

```bibtex
@article{gammelli2022graph,
  title={Graph neural network reinforcement learning for autonomous mobility-on-demand systems},
  author={Gammelli, Daniele and Yang, Kaidi and Harrison, James and Rodrigues, Filipe and Pereira, Francisco C and Pavone, Marco},
  journal={arXiv preprint arXiv:2104.11434},
  year={2022}
}
```

---

## ğŸ“§ Contact

- Original Authors: gammelli@stanford.edu, csasc@dtu.dk, ltresca@stanford.edu
- This Fork: effectusMOA (effectus60@naver.com)